{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "The third notebook in the pipeline.\n",
    "\n",
    "In this notebook, we utilize the enriched datasets to train the first (and possibly second) stage of the AI. For this notebook to run properly, use the following **exact** package versions:\n",
    "\n",
    "```\n",
    "!pip3 install -q -U bitsandbytes==0.42.0\n",
    "!pip3 install -q -U peft==0.8.2\n",
    "!pip3 install -q -U trl==0.7.10\n",
    "!pip3 install -q -U accelerate==0.27.1\n",
    "!pip3 install -q -U datasets==2.17.0\n",
    "!pip3 install -q -U transformers==4.38.0\n",
    "```\n",
    "\n",
    "because bitsandbytes, accelerate are bitch packages to work with.\n",
    "\n",
    "-------------------\n",
    "\n",
    "Useful resources:\n",
    "- https://huggingface.co/blog/gemma-peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/staff_homes/kboenisc/miniconda3/envs/baltasar-ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import polars as pl\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from trl import SFTTrainer\n",
    "from transformers import BitsAndBytesConfig\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    DATASET_PATH = 'src/the_art_of_worldly_wisdom_enriched_v2.json'\n",
    "    MODEL_ID = 'google/gemma-7b-it'\n",
    "    DEVICE = 'cuda:0'\n",
    "    FINE_TUNED_MODEL = 'google-gemma-7b-it-test'\n",
    "    HF_TOKEN = ''\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = Config.HF_TOKEN \n",
    "\n",
    "# set the qunatization configs\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/staff_homes/kboenisc/miniconda3/envs/baltasar-ai/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|███| 4/4 [00:08<00:00,  2.03s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(Config.MODEL_ID, quantization_config=bnb_config, device_map={\"\":0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_ID, add_eos_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup State-of-the-art Parameter-Efficient Fine-Tuning (PEFT) methods\n",
    "\n",
    "Sources:\n",
    "- https://github.com/huggingface/peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GemmaForCausalLM(\n",
      "  (model): GemmaModel(\n",
      "    (embed_tokens): Embedding(256000, 3072, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x GemmaDecoderLayer(\n",
      "        (self_attn): GemmaSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=3072, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=3072, out_features=4096, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=3072, out_features=4096, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=3072, bias=False)\n",
      "          (rotary_emb): GemmaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): GemmaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=3072, out_features=24576, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=3072, out_features=24576, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=24576, out_features=3072, bias=False)\n",
      "          (act_fn): GELUActivation()\n",
      "        )\n",
      "        (input_layernorm): GemmaRMSNorm()\n",
      "        (post_attention_layernorm): GemmaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): GemmaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3072, out_features=256000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['down_proj', 'v_proj', 'q_proj', 'o_proj', 'up_proj', 'gate_proj', 'k_proj']\n"
     ]
    }
   ],
   "source": [
    "def find_all_linear_names(model):\n",
    "  cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "  lora_module_names = set()\n",
    "  for name, module in model.named_modules():\n",
    "    if isinstance(module, cls):\n",
    "      names = name.split('.')\n",
    "      lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:\n",
    "      lora_module_names.remove('lm_head')\n",
    "  return list(lora_module_names)\n",
    "\n",
    "modules = find_all_linear_names(model)\n",
    "print(modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=32,\n",
    "    target_modules=modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GemmaForCausalLM(\n",
      "      (model): GemmaModel(\n",
      "        (embed_tokens): Embedding(256000, 3072, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-27): 28 x GemmaDecoderLayer(\n",
      "            (self_attn): GemmaSdpaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3072, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3072, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3072, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=3072, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): GemmaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): GemmaMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3072, out_features=24576, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=24576, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3072, out_features=24576, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=24576, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=24576, out_features=3072, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=24576, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): GELUActivation()\n",
      "            )\n",
      "            (input_layernorm): GemmaRMSNorm()\n",
      "            (post_attention_layernorm): GemmaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): GemmaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=3072, out_features=256000, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, lora_config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainable parameters: *(The amount of trainable parameters increases with a higher lora r)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 200015872 | total: 8737696768 | Percentage: 2.2891%\n"
     ]
    }
   ],
   "source": [
    "trainable, total = model.get_nb_trainable_parameters()\n",
    "print(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>src</th>\n",
       "      <th>count</th>\n",
       "      <th>header</th>\n",
       "      <th>content</th>\n",
       "      <th>instructions</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ger</td>\n",
       "      <td>https://www.projekt-gutenberg.org/gracian/orak...</td>\n",
       "      <td>1</td>\n",
       "      <td>Alles hat heut zu Tage seinen Gipfel erreicht,</td>\n",
       "      <td>aber die Kunst sich geltend zu machen, den höc...</td>\n",
       "      <td>Was denkst du über die Herausforderungen, vor ...</td>\n",
       "      <td>Die Herausforderungen, vor denen weise Mensche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ger</td>\n",
       "      <td>https://www.projekt-gutenberg.org/gracian/orak...</td>\n",
       "      <td>1</td>\n",
       "      <td>Alles hat heut zu Tage seinen Gipfel erreicht,</td>\n",
       "      <td>aber die Kunst sich geltend zu machen, den höc...</td>\n",
       "      <td>Was sind die Fähigkeiten oder Eigenschaften, d...</td>\n",
       "      <td>Ein weiser Mensch der heutigen Zeit benötigt, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ger</td>\n",
       "      <td>https://www.projekt-gutenberg.org/gracian/orak...</td>\n",
       "      <td>1</td>\n",
       "      <td>Alles hat heut zu Tage seinen Gipfel erreicht,</td>\n",
       "      <td>aber die Kunst sich geltend zu machen, den höc...</td>\n",
       "      <td>Wie gelingt es einem weisen Menschen, sich in ...</td>\n",
       "      <td>Ein weiser Mensch vollbringt seine Taten mit B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ger</td>\n",
       "      <td>https://www.projekt-gutenberg.org/gracian/orak...</td>\n",
       "      <td>1</td>\n",
       "      <td>Alles hat heut zu Tage seinen Gipfel erreicht,</td>\n",
       "      <td>aber die Kunst sich geltend zu machen, den höc...</td>\n",
       "      <td>Was denkst du über die steigenden Erwartungen ...</td>\n",
       "      <td>Die steigenden Erwartungen an die Intelligenz ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ger</td>\n",
       "      <td>https://www.projekt-gutenberg.org/gracian/orak...</td>\n",
       "      <td>2</td>\n",
       "      <td>Herz und Kopf:</td>\n",
       "      <td>die beiden Pole der Sonne unserer Fähigkeiten:...</td>\n",
       "      <td>Wie können wir unser Denken und Fühlen besser ...</td>\n",
       "      <td>Wohlgepflegtes Denken und Fühlen sind der Schl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang                                                src  count  \\\n",
       "0  ger  https://www.projekt-gutenberg.org/gracian/orak...      1   \n",
       "1  ger  https://www.projekt-gutenberg.org/gracian/orak...      1   \n",
       "2  ger  https://www.projekt-gutenberg.org/gracian/orak...      1   \n",
       "3  ger  https://www.projekt-gutenberg.org/gracian/orak...      1   \n",
       "4  ger  https://www.projekt-gutenberg.org/gracian/orak...      2   \n",
       "\n",
       "                                           header  \\\n",
       "0  Alles hat heut zu Tage seinen Gipfel erreicht,   \n",
       "1  Alles hat heut zu Tage seinen Gipfel erreicht,   \n",
       "2  Alles hat heut zu Tage seinen Gipfel erreicht,   \n",
       "3  Alles hat heut zu Tage seinen Gipfel erreicht,   \n",
       "4                                  Herz und Kopf:   \n",
       "\n",
       "                                             content  \\\n",
       "0  aber die Kunst sich geltend zu machen, den höc...   \n",
       "1  aber die Kunst sich geltend zu machen, den höc...   \n",
       "2  aber die Kunst sich geltend zu machen, den höc...   \n",
       "3  aber die Kunst sich geltend zu machen, den höc...   \n",
       "4  die beiden Pole der Sonne unserer Fähigkeiten:...   \n",
       "\n",
       "                                        instructions  \\\n",
       "0  Was denkst du über die Herausforderungen, vor ...   \n",
       "1  Was sind die Fähigkeiten oder Eigenschaften, d...   \n",
       "2  Wie gelingt es einem weisen Menschen, sich in ...   \n",
       "3  Was denkst du über die steigenden Erwartungen ...   \n",
       "4  Wie können wir unser Denken und Fühlen besser ...   \n",
       "\n",
       "                                              output  \n",
       "0  Die Herausforderungen, vor denen weise Mensche...  \n",
       "1  Ein weiser Mensch der heutigen Zeit benötigt, ...  \n",
       "2  Ein weiser Mensch vollbringt seine Taten mit B...  \n",
       "3  Die steigenden Erwartungen an die Intelligenz ...  \n",
       "4  Wohlgepflegtes Denken und Fühlen sind der Schl...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2396\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(Config.DATASET_PATH)\n",
    "display(df.head())\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(example):\n",
    "\n",
    "    begin = 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n'\n",
    "    instruct = '### Instruct: Answer or comment on the given input below as Baltasar Gracian, a 17th century philosopher.\\n\\n'\n",
    "    question = f\"### Input:\\n {example['instructions']}\\n\\n\"\n",
    "    output = f\"### Output:\\n {example['output']}\\n\\n\"\n",
    "    end = ''\n",
    "    return begin + instruct + question + output + end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruct: Answer or comment on the given input below as Baltasar Gracian, a 17th century philosopher.\n",
      "\n",
      "### Input:\n",
      " How can we evaluate the true value of a work or a person's talents—should we focus more on their depth and quality rather than their quantity?\n",
      "\n",
      "### Output:\n",
      " The true value of a work or a person's talents is best gauged by their depth and quality, for excellence doth reside in the rare and the profound, rather than the plentiful. Indeed, the best is always few and far between; a multitude devalues the singularly sublime. To seek quantity is to court mediocrity, whilst true eminence lies in the intensity of labor—a measure that lifts one to the heights of the heroic. Thus, let us prize that which is intense above that which is extensive, for in the finite shall we find the profound.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(get_prompt(df.iloc[1300]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inference_prompt(example):\n",
    "\n",
    "    begin = 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n'\n",
    "    instruct = '### Instruct: Answer or comment on the given input below as Baltasar Gracian, a 17th century philosopher.\\n\\n'\n",
    "    question = f\"### Input:\\n {example['instructions']}\\n\\n\"\n",
    "    output = f\"### Output:\\n\"\n",
    "    end = ''\n",
    "    return begin + instruct + question + output + end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    return [get_prompt(example)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model before fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruct: Answer or comment on the given input below as Baltasar Gracian, a 17th century philosopher.\n",
      "\n",
      "### Input:\n",
      " How can we evaluate the true value of a work or a person's talents—should we focus more on their depth and quality rather than their quantity?\n",
      "\n",
      "### Output:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(get_inference_prompt(df.iloc[1300]), return_tensors=\"pt\").to(Config.DEVICE)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkeboen\u001b[0m (\u001b[33mkeboen-ttlab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/staff_homes/kboenisc/home/baltasar-gracian-ai/baltasar-gracian-ai/wandb/run-20240917_180504-nyxhyeot</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/keboen-ttlab/Baltasar-Gracian-AI/runs/nyxhyeot/workspace' target=\"_blank\">gentle-wildflower-18</a></strong> to <a href='https://wandb.ai/keboen-ttlab/Baltasar-Gracian-AI' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/keboen-ttlab/Baltasar-Gracian-AI' target=\"_blank\">https://wandb.ai/keboen-ttlab/Baltasar-Gracian-AI</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/keboen-ttlab/Baltasar-Gracian-AI/runs/nyxhyeot/workspace' target=\"_blank\">https://wandb.ai/keboen-ttlab/Baltasar-Gracian-AI/runs/nyxhyeot/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/keboen-ttlab/Baltasar-Gracian-AI/runs/nyxhyeot?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fa51409fa90>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"Baltasar-Gracian-AI\", entity=\"keboen-ttlab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/staff_homes/kboenisc/miniconda3/envs/baltasar-ai/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|████████████| 2396/2396 [00:49<00:00, 48.21 examples/s]\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/baltasar-ai/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:290: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/baltasar-ai/lib/python3.10/site-packages/accelerate/accelerator.py:450: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    max_seq_length=512,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        # eval_accumulation_steps=1,\n",
    "        warmup_steps=2,\n",
    "        max_steps=25,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir='outputs',\n",
    "        optim='paged_adamw_8bit',\n",
    "        report_to='wandb'\n",
    "    ),\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/baltasar-ai/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/baltasar-ai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/baltasar-ai/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 07:22, Epoch 25/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.511100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.488000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.049500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.576900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.354900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.126600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.902200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.704100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.517000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.371400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.233900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.155300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.118500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.076400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.048200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.029200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.018100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.013100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.008800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.006500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.005200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.004200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.004400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.002200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=25, training_loss=0.5731383980531245, metrics={'train_runtime': 463.9783, 'train_samples_per_second': 0.431, 'train_steps_per_second': 0.054, 'total_flos': 1831971402547200.0, 'train_loss': 0.5731383980531245, 'epoch': 25.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('models/' + Config.FINE_TUNED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruct: Answer or comment on the given input below as Baltasar Gracian, a 17th century philosopher.\n",
      "\n",
      "### Input:\n",
      " How can we evaluate the true value of a work or a person's talents—should we focus more on their depth and quality rather than their quantity?\n",
      "\n",
      "### Output:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "prompt = get_inference_prompt(df.iloc[1300])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt, return_tensors='pt').to(Config.DEVICE)\n",
    "print(len(inputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruct: Answer or comment on the given input below as Baltasar Gracian, a 17th century philosopher.\n",
      "\n",
      "### Input:\n",
      " How can we evaluate the true value of a work or a person's talents—should we focus more on their depth and quality rather than their quantity?\n",
      "\n",
      "### Output:\n",
      "<eos>In evaluating a person or work, it is essential to consider both their depth and quality, rather than solely relying on their quantity. While quantity can be impressive, it is ultimately superficial. To truly gauge a person's talents or the quality of a work, we must delve into their innermost layers, exploring their depth and substance. Therefore, it is wiser to focus on the intrinsic worth of a thing, rather than its outward facade. In doing so, we can uncover the true treasures that lie hidden within.<eos>\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(**inputs, max_length=512)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
