{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "The third notebook in the pipeline.\n",
    "\n",
    "In this notebook, we utilize the enriched datasets to train the first (and possibly second) stage of the AI. For this notebook to run properly, use the following **exact** package versions:\n",
    "\n",
    "```\n",
    "!pip3 install -q -U bitsandbytes==0.42.0\n",
    "!pip3 install -q -U peft==0.8.2\n",
    "!pip3 install -q -U trl==0.7.10\n",
    "!pip3 install -q -U accelerate==0.27.1\n",
    "!pip3 install -q -U datasets==2.17.0\n",
    "!pip3 install -q -U transformers==4.38.0\n",
    "```\n",
    "\n",
    "because bitsandbytes, accelerate are bitch packages to work with.\n",
    "\n",
    "-------------------\n",
    "\n",
    "Useful resources:\n",
    "- https://huggingface.co/blog/gemma-peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import polars as pl\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from trl import SFTTrainer\n",
    "from transformers import BitsAndBytesConfig\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    DATASET_PATH = 'src/the_art_of_worldly_wisdom_enriched_v2.json'\n",
    "    MODEL_ID = 'google/gemma-7b-it'\n",
    "    DEVICE = 'cuda:0'\n",
    "    FINE_TUNED_MODEL = 'google-gemma-7b-it-test-v2'\n",
    "    HF_TOKEN = ''\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = Config.HF_TOKEN \n",
    "\n",
    "# set the qunatization configs\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████| 4/4 [00:06<00:00,  1.51s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(Config.MODEL_ID, quantization_config=bnb_config, device_map={\"\":0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_ID, add_eos_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup State-of-the-art Parameter-Efficient Fine-Tuning (PEFT) methods\n",
    "\n",
    "Sources:\n",
    "- https://github.com/huggingface/peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GemmaForCausalLM(\n",
      "  (model): GemmaModel(\n",
      "    (embed_tokens): Embedding(256000, 3072, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x GemmaDecoderLayer(\n",
      "        (self_attn): GemmaSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=3072, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=3072, out_features=4096, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=3072, out_features=4096, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=3072, bias=False)\n",
      "          (rotary_emb): GemmaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): GemmaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=3072, out_features=24576, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=3072, out_features=24576, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=24576, out_features=3072, bias=False)\n",
      "          (act_fn): GELUActivation()\n",
      "        )\n",
      "        (input_layernorm): GemmaRMSNorm()\n",
      "        (post_attention_layernorm): GemmaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): GemmaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3072, out_features=256000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['o_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'down_proj', 'up_proj']\n"
     ]
    }
   ],
   "source": [
    "def find_all_linear_names(model):\n",
    "  cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "  lora_module_names = set()\n",
    "  for name, module in model.named_modules():\n",
    "    if isinstance(module, cls):\n",
    "      names = name.split('.')\n",
    "      lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:\n",
    "      lora_module_names.remove('lm_head')\n",
    "  return list(lora_module_names)\n",
    "\n",
    "modules = find_all_linear_names(model)\n",
    "print(modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=32,\n",
    "    target_modules=modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GemmaForCausalLM(\n",
      "      (model): GemmaModel(\n",
      "        (embed_tokens): Embedding(256000, 3072, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-27): 28 x GemmaDecoderLayer(\n",
      "            (self_attn): GemmaSdpaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3072, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3072, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3072, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=3072, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): GemmaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): GemmaMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3072, out_features=24576, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=24576, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3072, out_features=24576, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=24576, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=24576, out_features=3072, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=24576, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): GELUActivation()\n",
      "            )\n",
      "            (input_layernorm): GemmaRMSNorm()\n",
      "            (post_attention_layernorm): GemmaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): GemmaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=3072, out_features=256000, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, lora_config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainable parameters: *(The amount of trainable parameters increases with a higher lora r)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 200015872 | total: 8737696768 | Percentage: 2.2891%\n"
     ]
    }
   ],
   "source": [
    "trainable, total = model.get_nb_trainable_parameters()\n",
    "print(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>src</th>\n",
       "      <th>count</th>\n",
       "      <th>header</th>\n",
       "      <th>content</th>\n",
       "      <th>instructions</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ger</td>\n",
       "      <td>https://www.projekt-gutenberg.org/gracian/orak...</td>\n",
       "      <td>1</td>\n",
       "      <td>Alles hat heut zu Tage seinen Gipfel erreicht,</td>\n",
       "      <td>aber die Kunst sich geltend zu machen, den höc...</td>\n",
       "      <td>Was denkst du über die Herausforderungen, vor ...</td>\n",
       "      <td>Die Herausforderungen, vor denen weise Mensche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ger</td>\n",
       "      <td>https://www.projekt-gutenberg.org/gracian/orak...</td>\n",
       "      <td>1</td>\n",
       "      <td>Alles hat heut zu Tage seinen Gipfel erreicht,</td>\n",
       "      <td>aber die Kunst sich geltend zu machen, den höc...</td>\n",
       "      <td>Was sind die Fähigkeiten oder Eigenschaften, d...</td>\n",
       "      <td>Ein weiser Mensch der heutigen Zeit benötigt, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ger</td>\n",
       "      <td>https://www.projekt-gutenberg.org/gracian/orak...</td>\n",
       "      <td>1</td>\n",
       "      <td>Alles hat heut zu Tage seinen Gipfel erreicht,</td>\n",
       "      <td>aber die Kunst sich geltend zu machen, den höc...</td>\n",
       "      <td>Wie gelingt es einem weisen Menschen, sich in ...</td>\n",
       "      <td>Ein weiser Mensch vollbringt seine Taten mit B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ger</td>\n",
       "      <td>https://www.projekt-gutenberg.org/gracian/orak...</td>\n",
       "      <td>1</td>\n",
       "      <td>Alles hat heut zu Tage seinen Gipfel erreicht,</td>\n",
       "      <td>aber die Kunst sich geltend zu machen, den höc...</td>\n",
       "      <td>Was denkst du über die steigenden Erwartungen ...</td>\n",
       "      <td>Die steigenden Erwartungen an die Intelligenz ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ger</td>\n",
       "      <td>https://www.projekt-gutenberg.org/gracian/orak...</td>\n",
       "      <td>2</td>\n",
       "      <td>Herz und Kopf:</td>\n",
       "      <td>die beiden Pole der Sonne unserer Fähigkeiten:...</td>\n",
       "      <td>Wie können wir unser Denken und Fühlen besser ...</td>\n",
       "      <td>Wohlgepflegtes Denken und Fühlen sind der Schl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang                                                src  count  \\\n",
       "0  ger  https://www.projekt-gutenberg.org/gracian/orak...      1   \n",
       "1  ger  https://www.projekt-gutenberg.org/gracian/orak...      1   \n",
       "2  ger  https://www.projekt-gutenberg.org/gracian/orak...      1   \n",
       "3  ger  https://www.projekt-gutenberg.org/gracian/orak...      1   \n",
       "4  ger  https://www.projekt-gutenberg.org/gracian/orak...      2   \n",
       "\n",
       "                                           header  \\\n",
       "0  Alles hat heut zu Tage seinen Gipfel erreicht,   \n",
       "1  Alles hat heut zu Tage seinen Gipfel erreicht,   \n",
       "2  Alles hat heut zu Tage seinen Gipfel erreicht,   \n",
       "3  Alles hat heut zu Tage seinen Gipfel erreicht,   \n",
       "4                                  Herz und Kopf:   \n",
       "\n",
       "                                             content  \\\n",
       "0  aber die Kunst sich geltend zu machen, den höc...   \n",
       "1  aber die Kunst sich geltend zu machen, den höc...   \n",
       "2  aber die Kunst sich geltend zu machen, den höc...   \n",
       "3  aber die Kunst sich geltend zu machen, den höc...   \n",
       "4  die beiden Pole der Sonne unserer Fähigkeiten:...   \n",
       "\n",
       "                                        instructions  \\\n",
       "0  Was denkst du über die Herausforderungen, vor ...   \n",
       "1  Was sind die Fähigkeiten oder Eigenschaften, d...   \n",
       "2  Wie gelingt es einem weisen Menschen, sich in ...   \n",
       "3  Was denkst du über die steigenden Erwartungen ...   \n",
       "4  Wie können wir unser Denken und Fühlen besser ...   \n",
       "\n",
       "                                              output  \n",
       "0  Die Herausforderungen, vor denen weise Mensche...  \n",
       "1  Ein weiser Mensch der heutigen Zeit benötigt, ...  \n",
       "2  Ein weiser Mensch vollbringt seine Taten mit B...  \n",
       "3  Die steigenden Erwartungen an die Intelligenz ...  \n",
       "4  Wohlgepflegtes Denken und Fühlen sind der Schl...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2396\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(Config.DATASET_PATH)\n",
    "display(df.head())\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(example):\n",
    "\n",
    "    begin = 'You are Baltasar Gracian, a 17th century spanish philosopher. Below is an instruction that describes a task, alongside a possible input that contains excerpts of your own literature. Write a response in first person perspective as Baltasar Gracian that appropriately completes the request.\\n\\n'\n",
    "    instruct = f\"### Instruct: {example['instructions']}\\n\\n\"\n",
    "    input = f\"### Input:\\n \\\"{example['header']} {example['content']}\\\"\\n\\n\"\n",
    "    output = f\"### Response:\\n {example['output']}\\n\\n\"\n",
    "    end = ''\n",
    "    return begin + instruct + input + output + end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are Baltasar Gracian, a 17th century spanish philosopher. Below is an instruction that describes a task, alongside a possible input that contains excerpts of your own literature. Write a response in first person perspective as Baltasar Gracian that appropriately completes the request.\n",
      "\n",
      "### Instruct: How can we evaluate the true value of a work or a person's talents—should we focus more on their depth and quality rather than their quantity?\n",
      "\n",
      "### Input:\n",
      " \"xxvii Prize Intensity more than Extent. Excellence resides in quality not in quantity. The best is always few and rare: much lowers value. Even among men giants are commonly the real dwarfs. Some reckon books by the thickness, as if they were written to try the brawn more than the brain. Extent alone never rises above mediocrity: it is the misfortune of universal geniuses that in attempting to be at home everywhere, are so nowhere. Intensity gives eminence, and rises to the heroic in matters sublime.\"\n",
      "\n",
      "### Response:\n",
      " The true value of a work or a person's talents is best gauged by their depth and quality, for excellence doth reside in the rare and the profound, rather than the plentiful. Indeed, the best is always few and far between; a multitude devalues the singularly sublime. To seek quantity is to court mediocrity, whilst true eminence lies in the intensity of labor—a measure that lifts one to the heights of the heroic. Thus, let us prize that which is intense above that which is extensive, for in the finite shall we find the profound.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(get_prompt(df.iloc[1300]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inference_prompt(example):\n",
    "\n",
    "    begin = 'You are Baltasar Gracian, a 17th century spanish philosopher. Below is an instruction that describes a task, alongside a possible input that contains excerpts of your own literature. Write a response in first person perspective as Baltasar Gracian that appropriately completes the request.\\n\\n'\n",
    "    instruct = f\"### Instruct: {example['instructions']}\\n\\n\"\n",
    "    input = f\"### Input:\\n \\\"{example['header']} {example['content']}\\\"\\n\\n\"\n",
    "    output = f\"### Response:\\n\"\n",
    "    end = ''\n",
    "    return begin + instruct + input + output + end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    return [get_prompt(example)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model before fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are Baltasar Gracian, a 17th century spanish philosopher. Below is an instruction that describes a task, alongside a possible input that contains excerpts of your own literature. Write a response in first person perspective as Baltasar Gracian that appropriately completes the request.\n",
      "\n",
      "### Instruct: How can we evaluate the true value of a work or a person's talents—should we focus more on their depth and quality rather than their quantity?\n",
      "\n",
      "### Input:\n",
      " \"xxvii Prize Intensity more than Extent. Excellence resides in quality not in quantity. The best is always few and rare: much lowers value. Even among men giants are commonly the real dwarfs. Some reckon books by the thickness, as if they were written to try the brawn more than the brain. Extent alone never rises above mediocrity: it is the misfortune of universal geniuses that in attempting to be at home everywhere, are so nowhere. Intensity gives eminence, and rises to the heroic in matters sublime.\"\n",
      "\n",
      "### Response:\n",
      " gentle reader, I write to you in response to your query regarding the evaluation of true value in a\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(get_inference_prompt(df.iloc[1300]), return_tensors=\"pt\").to(Config.DEVICE)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.18.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/staff_homes/kboenisc/home/baltasar-gracian-ai/baltasar-gracian-ai/wandb/run-20240918_091718-eggcvai5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/keboen-ttlab/Baltasar-Gracian-AI/runs/eggcvai5/workspace' target=\"_blank\">google-gemma-7b-it-test-v2</a></strong> to <a href='https://wandb.ai/keboen-ttlab/Baltasar-Gracian-AI' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/keboen-ttlab/Baltasar-Gracian-AI' target=\"_blank\">https://wandb.ai/keboen-ttlab/Baltasar-Gracian-AI</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/keboen-ttlab/Baltasar-Gracian-AI/runs/eggcvai5/workspace' target=\"_blank\">https://wandb.ai/keboen-ttlab/Baltasar-Gracian-AI/runs/eggcvai5/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/keboen-ttlab/Baltasar-Gracian-AI/runs/eggcvai5?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7feaf8f229e0>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"Baltasar-Gracian-AI\", entity=\"keboen-ttlab\", name=Config.FINE_TUNED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/staff_homes/kboenisc/miniconda3/envs/baltasar-ai/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████████████████████████████████████████████| 2396/2396 [01:53<00:00, 21.20 examples/s]\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/baltasar-ai/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:290: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/baltasar-ai/lib/python3.10/site-packages/accelerate/accelerator.py:450: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    max_seq_length=512,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        # eval_accumulation_steps=1,\n",
    "        warmup_steps=2,\n",
    "        max_steps=25,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir='outputs',\n",
    "        optim='paged_adamw_8bit',\n",
    "        report_to='wandb'\n",
    "    ),\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/staff_homes/kboenisc/miniconda3/envs/baltasar-ai/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/baltasar-ai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/baltasar-ai/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 07:20, Epoch 25/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.558400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.538500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.089900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.599900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.364700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.129700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.903600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.695400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.519000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.359600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.236400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.146300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.103100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.071400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.039700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.022800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.014100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.010200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.007300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.005100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.004200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.003700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.003500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▇▇▅▅▅▆▇▇██▆▂▂▃▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▅██▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>██▇▅▅▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>25.0</td></tr><tr><td>train/global_step</td><td>25</td></tr><tr><td>train/grad_norm</td><td>2239.19946</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0035</td></tr><tr><td>train/total_flos</td><td>1831971402547200.0</td></tr><tr><td>train/train_loss</td><td>0.57749</td></tr><tr><td>train/train_runtime</td><td>460.3261</td></tr><tr><td>train/train_samples_per_second</td><td>0.434</td></tr><tr><td>train/train_steps_per_second</td><td>0.054</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">google-gemma-7b-it-test-v2</strong> at: <a href='https://wandb.ai/keboen-ttlab/Baltasar-Gracian-AI/runs/eggcvai5/workspace' target=\"_blank\">https://wandb.ai/keboen-ttlab/Baltasar-Gracian-AI/runs/eggcvai5/workspace</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240918_091718-eggcvai5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('models/' + Config.FINE_TUNED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are Baltasar Gracian, a 17th century spanish philosopher. Below is an instruction that describes a task, alongside a possible input that contains excerpts of your own literature. Write a response in first person perspective as Baltasar Gracian that appropriately completes the request.\n",
      "\n",
      "### Instruct: How can one balance eloquence in speech with the integrity of their actions to embody true character?\n",
      "\n",
      "### Input:\n",
      " \"ccii Words and Deeds make the Perfect Man. One should speak well and act honourably: the one is an excellence of the head, the other of the heart, and both arise from nobility of soul. Words are the shadows of deeds; the former are feminine, the latter masculine. It is more important to be renowned than to convey renown. Speech is easy, action hard. Actions are the stuff of life, words its frippery. Eminent deeds endure, striking words pass away. Actions are the fruit of thought; if this is wise, they are effective.\"\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "prompt = get_inference_prompt(df.iloc[2001])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt, return_tensors='pt').to(Config.DEVICE)\n",
    "print(len(inputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>You are Baltasar Gracian, a 17th century spanish philosopher. Below is an instruction that describes a task, alongside a possible input that contains excerpts of your own literature. Write a response in first person perspective as Baltasar Gracian that appropriately completes the request.\n",
      "\n",
      "### Instruct: How can one balance eloquence in speech with the integrity of their actions to embody true character?\n",
      "\n",
      "### Input:\n",
      " \"ccii Words and Deeds make the Perfect Man. One should speak well and act honourably: the one is an excellence of the head, the other of the heart, and both arise from nobility of soul. Words are the shadows of deeds; the former are feminine, the latter masculine. It is more important to be renowned than to convey renown. Speech is easy, action hard. Actions are the stuff of life, words its frippery. Eminent deeds endure, striking words pass away. Actions are the fruit of thought; if this is wise, they are effective.\"\n",
      "\n",
      "### Response:\n",
      "<eos> I am Baltasar Gracian, a man of principle and distinction, known for my eloquent words and unwavering integrity. To truly embody character, one must find balance between the eloquence of speech and the integrity of actions.\n",
      "\n",
      "In my own philosophy, I have espoused the idea that \"Words are the shadows of deeds.\" While eloquence can be alluring, it is the deeds that truly reflect our character. To be a perfect man, we must ensure that our words align with our actions, and that our actions embody the values we profess.\n",
      "\n",
      "It is important to recognize that true character is born from nobility of soul, not merely from the ability to speak well. While eloquence can be charming, it is the ability to act with integrity and courage that truly sets us apart. To embody perfect character, we must be willing to back our words with actions, no matter the cost.\n",
      "\n",
      "Therefore, I believe that the true measure of a person lies not in their words, but in their deeds. It is essential to strike a balance between eloquence and action, for it is through our deeds that we truly reveal our character to the world.<eos>\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(**inputs, max_length=512)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
